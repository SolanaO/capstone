{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "## import operating system module\n",
    "import os\n",
    "\n",
    "## import module to measure the time\n",
    "from time import time\n",
    "\n",
    "## linear algebra package\n",
    "import numpy as np\n",
    "\n",
    "## data manipulation package\n",
    "import pandas as pd\n",
    "\n",
    "## visualizations packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "## the magic word for inline visualizations in Jupyter notebook\n",
    "% matplotlib inline\n",
    "\n",
    "## import module to process regular expressions\n",
    "import re\n",
    "\n",
    "## modules to split the data into training and testing sets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "## package to upload and find the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "## the list of stopwords to be used\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "## tokenizer package\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## import stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "## define a stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "## modules to split the data into training and testing sets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "## import the sentiment analyzer utility from nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "## import utilities to evaluate metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "## import deep learning libraries and packages\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "\n",
    "## packages to work with word vectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## package that contains the Word2Vec embedding\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import deep learning packages\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data from the cvs file and save it as a Pandas dataframe \n",
    "mobile = pd.read_csv('mobile_phones_prepared.csv') \n",
    "\n",
    "## make a work copy of the data\n",
    "df = mobile.copy()\n",
    "\n",
    "## check the dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function that performs all the steps neccessary for pre-processing of the reviews\n",
    "def clean_data(revseries):\n",
    "    ## rewrite all words in lower case form\n",
    "    revseries_clean  = revseries.apply(lambda x: x.lower())\n",
    "    ## remove special characters, punctuation and numbers - keep letters only\n",
    "    revseries_clean = revseries_clean.apply(lambda x: re.sub(\"[^a-zA-Z]+\",\" \", x))\n",
    "    ## remove stopwords\n",
    "    revseries_clean = revseries_clean.apply(lambda x: ' '.join(x for x in x.split() if x not in stop))\n",
    "    ## tokenize all reviews in the dataset\n",
    "    revseries_clean = revseries_clean.apply(lambda x: word_tokenize(x))\n",
    "    ## stem all reviews in the dataset\n",
    "    revseries_clean = revseries_clean.apply(lambda x: [stemmer.stem(w) for w in x])\n",
    "    return revseries_clean\n",
    "\n",
    "## rewrite the reviews as strings\n",
    "def clean_strings(revseries): \n",
    "    revseries_clean = revseries.apply(lambda x: ' '.join(x))\n",
    "    return revseries_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader Sentiment Analyzer in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a copy of the data \n",
    "sendata = mobile.copy()\n",
    "\n",
    "## drop unnecessary columns \n",
    "sendata = sendata.drop(['name', 'brand', 'price', 'votes', 'revl'], axis=1)\n",
    "\n",
    "## create a column that contains the cleaned reviews\n",
    "sendata['revs'] = clean_strings(clean_data(sendata['review']))\n",
    "\n",
    "## check for success\n",
    "sendata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define the analyzer \n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "## define a function that extracts the compund sentiment score\n",
    "def sentimental_value(review):\n",
    "    result = analyser.polarity_scores(review)['compound']\n",
    "    return round(result,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a column that records the compound sentiment score in the sample\n",
    "sendata['sentiment'] = sendata['revs'].apply(lambda x: sentimental_value(x))\n",
    "\n",
    "## check for success\n",
    "sendata.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a new column that bins the sentiment scores: score >= 0 being positive (or 1)\n",
    "sendata['sen_bin'] = sendata['sentiment'].apply(lambda x: 1 if x >= 0 else 0) \n",
    "\n",
    "## check for success\n",
    "sendata.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare the scores obtained by binning together the ratings with those provided by the \n",
    "## binned sentiment analyzer compound scores\n",
    "\n",
    "print('Accuracy score: ', format(round(metrics.accuracy_score(sendata['feel'],sendata['sen_bin']), 2)))\n",
    "print('Precision score: ', format(round(metrics.precision_score(sendata['feel'],sendata['sen_bin']), 2)))\n",
    "print('Recall score: ', format(round(metrics.recall_score(sendata['feel'],sendata['sen_bin']), 2)))\n",
    "print('F1 score: ', format(round(metrics.f1_score(sendata['feel'],sendata['sen_bin']), 2)))\n",
    "print('AUC score: ', format(round(metrics.roc_auc_score(sendata['feel'],sendata['sen_bin']), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is unbalanced, in order to deal with this issue I will use the StratifiedShuffleSplit method. The data will be split into a training set (80% of data) and a testing set (20% of data). The features to be investigated are the review texts, while the labels are provided by the 'feel' column, which classifies the review as positive (1) or negative (0).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the features and labels vectors \n",
    "X = df['review']\n",
    "y = df['feel']\n",
    "\n",
    "## split the data with 80% training set and 20% testing set\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, train_size=0.8, random_state=42)\n",
    "sss.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create the train and the test sets of features and labels \n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for success\n",
    "X_train[:2], y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Word Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prepare the reviews for the supervised learner\n",
    "Xv_train = clean_strings(clean_data(X_train)) \n",
    "Xv_test =  clean_strings(clean_data(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a matrix of relative frequencies from the cleaned reviews\n",
    "\n",
    "## import and set the vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the vectorizer method to the train data\n",
    "vectorized_train_data = vectorizer.fit_transform(Xv_train)\n",
    "\n",
    "## get some information on the output\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print('Number of different words: {}'.format(len(feature_names)))\n",
    "print('Vectorizer shape: {}'.format(vectorized_train_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## apply the vectorizer method to the test data\n",
    "vectorized_test_data = vectorizer.transform(Xv_test)\n",
    "\n",
    "## get some information on the output\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print('Number of different words: {}'.format(len(feature_names)))\n",
    "print('Vectorizer shape: {}'.format(vectorized_test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the supervised learning model from sklearn\n",
    "from sklearn import svm\n",
    "## initialize the classifier\n",
    "clf_svm = svm.SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# fit the classifier on the training data, measure training time\n",
    "start = time() \n",
    "clf_svm.fit(vectorized_train_data, y_train)\n",
    "end=time()\n",
    "time_fit=end-start\n",
    "print('Training time is {} sec.'.format(round(time_fit,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make predictions on the test data and store them in a variable 'pred'\n",
    "## measure the prediction time\n",
    "\n",
    "startp = time() \n",
    "pred = clf_svm.predict(vectorized_test_data)\n",
    "endp=time()\n",
    "time_pred=endp-startp\n",
    "print('Prediction time is {} sec.'.format(round(time_pred,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate the model\n",
    "print('Accuracy score: ', format(round(metrics.accuracy_score(y_test,pred), 2)))\n",
    "print('Precision score: ', format(round(metrics.precision_score(y_test,pred), 2)))\n",
    "print('Recall score: ', format(round(metrics.recall_score(y_test,pred), 2)))\n",
    "print('F1 score: ', format(round(metrics.f1_score(y_test,pred), 2)))\n",
    "print('AUC score: ', format(round(metrics.roc_auc_score(y_test,pred), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix for SVM \n",
    "\n",
    "The confusion matrix for the svm_classifier (according to this [thread](https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the labels and create the confusion matrix\n",
    "cmatrix = metrics.confusion_matrix(y_test, pred)\n",
    "print('The confusion matrix:\\n')\n",
    "print(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the figure in which the matrix will be drawn\n",
    "fig = plt.figure(figsize=(10, 10), dpi=50, linewidth=2, frameon=True)\n",
    "\n",
    "## add the figure's details\n",
    "labels=[0,1]\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cmatrix)\n",
    "#plt.title('Confusion matrix of the svm classifier', fontsize=12)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels, fontsize=16)\n",
    "ax.set_yticklabels([''] + labels, fontsize=16)\n",
    "\n",
    "plt.xlabel('Predicted Values', fontsize=16)\n",
    "plt.ylabel('True Values', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the variables and compute the auc (area under curve)\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, pred)\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "## create the figure in which the matrix will be drawn\n",
    "fig = plt.figure(figsize=(10, 10), dpi=50, linewidth=2, frameon=True)\n",
    "\n",
    "## create title\n",
    "plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n",
    "\n",
    "## plot the curve\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "\n",
    "## intervals, ticks and labels for the two axes\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prepare the reviews for the unsupervised learner\n",
    "Xu_train = clean_data(X_train)\n",
    "Xu_test =  clean_data(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save the pre-processed reviews as a list of lists\n",
    "x_train_corpus = list(Xu_train)\n",
    "x_test_corpus = list(Xu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters for the model\n",
    "embedding_dim = 200 \n",
    "max_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences = x_train_corpus, window=4, workers=4,\n",
    "                               size = embedding_dim, min_count = 10)\n",
    "\n",
    "## create a vocabulary\n",
    "vocabulary = model.wv.vocab\n",
    "\n",
    "## the list of words in the vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabulary size is: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print several words in the vocabulary\n",
    "vocabulary_words = list(model.wv.vocab.keys())\n",
    "print(vocabulary_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the Word2Vec model\n",
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save the model to a file\n",
    "model.wv.save_word2vec_format('reviews_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## extract the word embeddings from the file \n",
    "## save the embeddings as a dictionary \n",
    "embedded_reviews = {}\n",
    "\n",
    "file = open('reviews_word2vec.txt')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    components = np.array(values[1:])\n",
    "    embedded_reviews[word] = components\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the word embeddings into a tokenized vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_corpus)\n",
    "\n",
    "## create a list vectors with integer components, one for each word\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train_corpus)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for success\n",
    "train_sequences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the number of words in the list created by the tokenizer\n",
    "train_word_index = tokenizer.word_index\n",
    "print('There are {} tokens.'.format(len(train_word_index)))\n",
    "\n",
    "## pad train sequences to the same length \n",
    "x_train_tensor = pad_sequences(train_sequences, maxlen=embedding_dim)\n",
    "print('Shape of train matrix: {}'.format(x_train_tensor.shape))\n",
    "\n",
    "## pad test sequences to have the same length\n",
    "x_test_tensor = pad_sequences(test_sequences, maxlen=embedding_dim)\n",
    "print('Shape of test matrix: {}'.format(x_test_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## map embeddings from Word2Vec model for each word to vocabulary\n",
    "## and create a matrix with word vectors\n",
    "\n",
    "num_words = len(train_word_index) + 1\n",
    "embedding_weights = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in train_word_index.items():\n",
    "    embedding_weights[i] = train_word_index.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the embedding matrix has nr.tokens x embedding_dim dimension\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the first CNN architecture\n",
    "\n",
    "model_first = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(num_words, embedding_dim,\n",
    "                            weights=[embedding_weights],\n",
    "                          input_length = embedding_dim, \n",
    "                            trainable = False )\n",
    "\n",
    "model_first.add(embedding_layer)\n",
    "\n",
    "model_first.add(Conv1D(filters=32,kernel_size=3, padding = 'same', activation='relu'))\n",
    "model_first.add(BatchNormalization())\n",
    "model_first.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "\n",
    "model_first.add(Conv1D(filters=64,kernel_size=4, padding = 'same', activation='relu'))\n",
    "model_first.add(BatchNormalization())\n",
    "model_first.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "model_first.add(Dropout(0.2))\n",
    "\n",
    "model_first.add(Conv1D(filters=128,kernel_size=5, padding = 'same', activation='relu'))\n",
    "model_first.add(BatchNormalization())\n",
    "model_first.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "model_first.add(Dropout(0.2))\n",
    "\n",
    "model_first.add(Flatten())\n",
    "model_first.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "### print the architecture of the CNN\n",
    "model_first.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plot a diagram of the model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model_first, to_file='model_first_plot.png',\n",
    "           show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## compile the model\n",
    "model_first.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "model_first_data= model_first.fit(x_train_tensor, y_train, batch_size=64, epochs=4, \n",
    "          validation_split=0.1, shuffle=True, verbose=2)\n",
    "end = time()\n",
    "\n",
    "duration = end-start\n",
    "print('The first model trained in {} sec.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model_first.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval=model_first.evaluate(x_test_tensor, y_test, verbose=2)\n",
    "print('Test loss:', round(test_eval[0],2))\n",
    "print('Test accuracy:', round(test_eval[1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_first_data.history['acc']\n",
    "val_accuracy = model_first_data.history['val_acc']\n",
    "loss = model_first_data.history['loss']\n",
    "val_loss = model_first_data.history['val_loss']\n",
    "\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'm', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## the updated CNN model\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(num_words, embedding_dim,\n",
    "                            weights=[embedding_weights],\n",
    "                          input_length = embedding_dim, \n",
    "                            trainable = False )\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(filters=100,kernel_size=2, padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=100,kernel_size=3, padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=100,kernel_size=4, padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=100,kernel_size=5, padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=100,kernel_size=6, padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "### print the architecture of the CNN\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot a diagram of the model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "## plot a diagram of the model\n",
    "plot_model(model, to_file='model_plot.png',\n",
    "           show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## fit the model\n",
    "start = time()\n",
    "model_data = model.fit(x_train_tensor, y_train, batch_size=50, epochs=10, \n",
    "          validation_split=0.1, shuffle=True, verbose=2)\n",
    "end = time()\n",
    "\n",
    "duration = end-start\n",
    "print('The model trained in {} sec.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"mymodel.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_m=model.evaluate(x_test_tensor, y_test, verbose=2)\n",
    "print('Test loss:', round(test_eval_m[0],2))\n",
    "print('Test accuracy:', round(test_eval_m[1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_data.history['acc']\n",
    "val_accuracy = model_data.history['val_acc']\n",
    "loss = model_data.history['loss']\n",
    "val_loss = model_data.history['val_loss']\n",
    "\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'm', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
